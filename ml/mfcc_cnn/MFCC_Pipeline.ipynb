{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **MFCC + CNN**"
      ],
      "metadata": {
        "id": "xIkvETim5E23"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lcWlhDJQ5aBh"
      },
      "outputs": [],
      "source": [
        "!pip install -q librosa tensorflow numpy scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sZjZ9xxv-s2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae6eb9d-a4bb-4669-a413-5250420dcc41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RAVDESS_DIR = \"/content/drive/MyDrive/datasets/ravdess\"\n",
        "ESC50_DIR   = \"/content/drive/MyDrive/datasets/esc50/audio\""
      ],
      "metadata": {
        "id": "JyZmG4eRoyIM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6NY_3N0Y-2Rg"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "SR = 16000\n",
        "DURATION = 1\n",
        "SAMPLES = SR * DURATION\n",
        "N_MFCC = 40\n",
        "\n",
        "def extract_mfcc(path):\n",
        "    audio, _ = librosa.load(path, sr=SR, mono=True)\n",
        "    audio = audio[:SAMPLES] if len(audio) > SAMPLES else np.pad(audio, (0, SAMPLES-len(audio)))\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=N_MFCC)\n",
        "    return mfcc.T\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "X, y = [], []\n",
        "\n",
        "for root, _, files in os.walk(RAVDESS_DIR):\n",
        "    for f in files:\n",
        "        if f.endswith(\".wav\"):\n",
        "            emotion = f.split(\"-\")[2]\n",
        "            label = 1 if emotion in [\"05\",\"06\",\"07\",\"08\"] else 0\n",
        "            X.append(extract_mfcc(os.path.join(root, f)))\n",
        "            y.append(label)\n"
      ],
      "metadata": {
        "id": "4WLMc2RKtksX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "THREAT_CLASSES = {10, 46, 49}  # scream, glass_breaking, alarm\n",
        "\n",
        "for f in os.listdir(ESC50_DIR):\n",
        "    if f.endswith(\".wav\"):\n",
        "        class_id = int(f.split(\"-\")[-1].replace(\".wav\",\"\"))\n",
        "        label = 1 if class_id in THREAT_CLASSES else 0\n",
        "        X.append(extract_mfcc(os.path.join(ESC50_DIR, f)))\n",
        "        y.append(label)\n"
      ],
      "metadata": {
        "id": "DLI2Rqzismu0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "X = X[..., np.newaxis]  # CNN channel\n",
        "print(X.shape, y.shape)\n"
      ],
      "metadata": {
        "id": "lZGJIdcgsoe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1012684e-178c-46fe-b482-e1441c2ac399"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4880, 32, 40, 1) (4880,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32,(3,3),activation='relu',input_shape=X.shape[1:]),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64,(3,3),activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64,activation='relu'),\n",
        "    layers.Dense(1,activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(X, y, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "b9QIggSGsq9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba4b274-325e-4be2-c6cc-ad496ff69534"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.6650 - loss: 1.2616\n",
            "Epoch 2/10\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7285 - loss: 0.5173\n",
            "Epoch 3/10\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7905 - loss: 0.4162\n",
            "Epoch 4/10\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8351 - loss: 0.3671\n",
            "Epoch 5/10\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8689 - loss: 0.2934\n",
            "Epoch 6/10\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9189 - loss: 0.2186\n",
            "Epoch 7/10\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9505 - loss: 0.1425\n",
            "Epoch 8/10\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9608 - loss: 0.1101\n",
            "Epoch 9/10\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9868 - loss: 0.0518\n",
            "Epoch 10/10\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9847 - loss: 0.0496\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e753f786c30>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"MFCC model exported\")\n"
      ],
      "metadata": {
        "id": "EDfMl-IMssx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7342ba3f-7391-4301-a730-0d82d394603e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp5qy807k6'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32, 40, 1), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  139041632357840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139041632358992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139041632359376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139041632358416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139041632358032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139041632359568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139041632356496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139041632358224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "MFCC model exported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "3b3c6864",
        "outputId": "6ed7012b-f450-4a0c-aae1-a79194d55dab"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"mfcc_audio_model.tflite\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3166bc24-d338-4fc0-8340-08192fa5ce48\", \"mfcc_audio_model.tflite\", 221968)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Good. Below is **clean, fully separated, production-correct code** for **TWO DISTINCT COLAB NOTEBOOKS**, exactly as you asked.\n",
        "\n",
        "# No mixing.\n",
        "# No shortcuts.\n",
        "# No hidden assumptions.\n",
        "\n",
        "# You can copy-paste each notebook as-is.\n",
        "\n",
        "# ---\n",
        "\n",
        "# # NOTEBOOK 1\n",
        "\n",
        "# ## **MFCC + CNN**\n",
        "\n",
        "# ### (RAVDESS + ESC-50 → MFCC features → CNN)\n",
        "\n",
        "# **Notebook name:**\n",
        "# `mfcc_cnn_training.ipynb`\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 0️⃣ SETUP\n",
        "\n",
        "# ```python\n",
        "# !pip install -q librosa tensorflow numpy scikit-learn\n",
        "# ```\n",
        "\n",
        "# ```python\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 1️⃣ PATHS (EXACT AS YOU REQUESTED)\n",
        "\n",
        "# ```python\n",
        "# RAVDESS_DIR = \"/content/drive/MyDrive/datasets/ravdess\"\n",
        "# ESC50_DIR   = \"/content/drive/MyDrive/datasets/esc50/audio\"\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 2️⃣ AUDIO → MFCC FUNCTION\n",
        "\n",
        "# ```python\n",
        "# import librosa\n",
        "# import numpy as np\n",
        "\n",
        "# SR = 16000\n",
        "# DURATION = 1\n",
        "# SAMPLES = SR * DURATION\n",
        "# N_MFCC = 40\n",
        "\n",
        "# def extract_mfcc(path):\n",
        "#     audio, _ = librosa.load(path, sr=SR, mono=True)\n",
        "#     audio = audio[:SAMPLES] if len(audio) > SAMPLES else np.pad(audio, (0, SAMPLES-len(audio)))\n",
        "#     mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=N_MFCC)\n",
        "#     return mfcc.T\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 3️⃣ LOAD RAVDESS (LABELS CORRECT)\n",
        "\n",
        "# ```python\n",
        "# import os\n",
        "\n",
        "# X, y = [], []\n",
        "\n",
        "# for root, _, files in os.walk(RAVDESS_DIR):\n",
        "#     for f in files:\n",
        "#         if f.endswith(\".wav\"):\n",
        "#             emotion = f.split(\"-\")[2]\n",
        "#             label = 1 if emotion in [\"05\",\"06\",\"07\",\"08\"] else 0\n",
        "#             X.append(extract_mfcc(os.path.join(root, f)))\n",
        "#             y.append(label)\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 4️⃣ LOAD ESC-50 (FILENAME-BASED)\n",
        "\n",
        "# ESC-50 naming:\n",
        "\n",
        "# ```\n",
        "# 1-13571-A-46.wav\n",
        "# ```\n",
        "\n",
        "# Class ID = last number before `.wav`\n",
        "\n",
        "# ```python\n",
        "# THREAT_CLASSES = {10, 46, 49}  # scream, glass_breaking, alarm\n",
        "\n",
        "# for f in os.listdir(ESC50_DIR):\n",
        "#     if f.endswith(\".wav\"):\n",
        "#         class_id = int(f.split(\"-\")[-1].replace(\".wav\",\"\"))\n",
        "#         label = 1 if class_id in THREAT_CLASSES else 0\n",
        "#         X.append(extract_mfcc(os.path.join(ESC50_DIR, f)))\n",
        "#         y.append(label)\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 5️⃣ PREPARE DATA\n",
        "\n",
        "# ```python\n",
        "# import numpy as np\n",
        "\n",
        "# X = np.array(X)\n",
        "# y = np.array(y)\n",
        "\n",
        "# X = X[..., np.newaxis]  # CNN channel\n",
        "# print(X.shape, y.shape)\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 6️⃣ MFCC + CNN MODEL\n",
        "\n",
        "# ```python\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers, models\n",
        "\n",
        "# model = models.Sequential([\n",
        "#     layers.Conv2D(32,(3,3),activation='relu',input_shape=X.shape[1:]),\n",
        "#     layers.MaxPooling2D((2,2)),\n",
        "#     layers.Conv2D(64,(3,3),activation='relu'),\n",
        "#     layers.MaxPooling2D((2,2)),\n",
        "#     layers.Flatten(),\n",
        "#     layers.Dense(64,activation='relu'),\n",
        "#     layers.Dense(1,activation='sigmoid')\n",
        "# ])\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer=\"adam\",\n",
        "#     loss=\"binary_crossentropy\",\n",
        "#     metrics=[\"accuracy\"]\n",
        "# )\n",
        "\n",
        "# model.fit(X, y, epochs=10, batch_size=32)\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 7️⃣ EXPORT MFCC MODEL → TFLITE\n",
        "\n",
        "# ```python\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# tflite_model = converter.convert()\n",
        "\n",
        "# with open(\"mfcc_audio_model.tflite\",\"wb\") as f:\n",
        "#     f.write(tflite_model)\n",
        "\n",
        "# print(\"MFCC model exported\")\n",
        "# ```\n",
        "\n",
        "# Upload:\n",
        "\n",
        "# ```\n",
        "# ml/exports/mfcc_audio_model.tflite\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# # NOTEBOOK 2\n",
        "\n",
        "# ## **Wav2Vec2BERT**\n",
        "\n",
        "# ### (RAVDESS + ESC-50 → RAW WAVEFORM → TRANSFORMER)\n",
        "\n",
        "# **Notebook name:**\n",
        "# `wav2vec2_training.ipynb`\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 0️⃣ SETUP\n",
        "\n",
        "# ```python\n",
        "# !pip install -q transformers torch torchaudio librosa tensorflow\n",
        "# ```\n",
        "\n",
        "# ```python\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 1️⃣ PATHS\n",
        "\n",
        "# ```python\n",
        "# RAVDESS_DIR = \"/content/drive/MyDrive/datasets/ravdess\"\n",
        "# ESC50_DIR   = \"/content/drive/MyDrive/datasets/esc50/audio\"\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 2️⃣ RAW AUDIO LOADER (CRITICAL)\n",
        "\n",
        "# ```python\n",
        "# import librosa\n",
        "# import numpy as np\n",
        "\n",
        "# SR = 16000\n",
        "# MAX_LEN = SR\n",
        "\n",
        "# def load_wave(path):\n",
        "#     audio, _ = librosa.load(path, sr=SR, mono=True)\n",
        "#     return audio[:MAX_LEN] if len(audio) > MAX_LEN else np.pad(audio,(0,MAX_LEN-len(audio)))\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 3️⃣ LOAD RAVDESS (RAW)\n",
        "\n",
        "# ```python\n",
        "# X_wave, y = [], []\n",
        "\n",
        "# import os\n",
        "# for root, _, files in os.walk(RAVDESS_DIR):\n",
        "#     for f in files:\n",
        "#         if f.endswith(\".wav\"):\n",
        "#             emotion = f.split(\"-\")[2]\n",
        "#             label = 1 if emotion in [\"05\",\"06\",\"07\",\"08\"] else 0\n",
        "#             X_wave.append(load_wave(os.path.join(root,f)))\n",
        "#             y.append(label)\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 4️⃣ LOAD ESC-50 (RAW)\n",
        "\n",
        "# ```python\n",
        "# THREAT_CLASSES = {10, 46, 49}\n",
        "\n",
        "# for f in os.listdir(ESC50_DIR):\n",
        "#     if f.endswith(\".wav\"):\n",
        "#         class_id = int(f.split(\"-\")[-1].replace(\".wav\",\"\"))\n",
        "#         label = 1 if class_id in THREAT_CLASSES else 0\n",
        "#         X_wave.append(load_wave(os.path.join(ESC50_DIR,f)))\n",
        "#         y.append(label)\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 5️⃣ PROCESS WITH WAV2VEC2 PROCESSOR\n",
        "\n",
        "# ```python\n",
        "# import torch\n",
        "# from transformers import Wav2Vec2Processor\n",
        "\n",
        "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "# inputs = processor(\n",
        "#     X_wave,\n",
        "#     sampling_rate=16000,\n",
        "#     return_tensors=\"pt\",\n",
        "#     padding=True\n",
        "# )\n",
        "\n",
        "# labels = torch.tensor(y)\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 6️⃣ TRAIN WAV2VEC2 (CORRECT)\n",
        "\n",
        "# ```python\n",
        "# from transformers import Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "#     \"facebook/wav2vec2-base\",\n",
        "#     num_labels=2\n",
        "# )\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "# model.train()\n",
        "# for epoch in range(3):\n",
        "#     optimizer.zero_grad()\n",
        "#     out = model(\n",
        "#         input_values=inputs.input_values,\n",
        "#         attention_mask=inputs.attention_mask,\n",
        "#         labels=labels\n",
        "#     )\n",
        "#     loss = out.loss\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     print(f\"Epoch {epoch} | Loss {loss.item():.4f}\")\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## 7️⃣ EXPORT WAV2VEC2 → TFLITE (INT8)\n",
        "\n",
        "# ```python\n",
        "# import tensorflow as tf\n",
        "# from transformers import TFWav2Vec2ForSequenceClassification\n",
        "\n",
        "# tf_model = TFWav2Vec2ForSequenceClassification.from_pretrained(\n",
        "#     model, from_pt=True\n",
        "# )\n",
        "\n",
        "# tf.saved_model.save(tf_model, \"wav2vec_saved\")\n",
        "\n",
        "# converter = tf.lite.TFLiteConverter.from_saved_model(\"wav2vec_saved\")\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.target_spec.supported_types = [tf.int8]\n",
        "\n",
        "# tflite = converter.convert()\n",
        "\n",
        "# with open(\"audio_model.tflite\",\"wb\") as f:\n",
        "#     f.write(tflite)\n",
        "\n",
        "# print(\"Wav2Vec2 model exported\")\n",
        "# ```\n",
        "\n",
        "# Upload:\n",
        "\n",
        "# ```\n",
        "# ml/exports/audio_model.tflite\n",
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# # FINAL VALIDATION (IMPORTANT)\n",
        "\n",
        "# ### Shapes MUST be:\n",
        "\n",
        "# * MFCC CNN input:\n",
        "\n",
        "#   ```\n",
        "#   (batch, time, mfcc, 1)\n",
        "#   ```\n",
        "# * Wav2Vec2 input:\n",
        "\n",
        "#   ```\n",
        "#   (batch, 16000)\n",
        "#   ```\n",
        "\n",
        "# No overlap.\n",
        "# No reuse.\n",
        "# No confusion.\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## NEXT (ONLY ONE ANSWER)\n",
        "\n",
        "# * `ANDROID_WIRING` → full Java integration of **both models**\n",
        "# * `YOLO` → vision pipeline\n",
        "# * `VIVA` → how to explain this cleanly in exam\n",
        "\n",
        "# Say the word.\n"
      ],
      "metadata": {
        "id": "j75YNqiV1a_k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}